/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
[32m[2024-06-19 13:50:54,295] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 13:50:54,319] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 13:50:54,320] [    INFO][0m - Loading configuration file model_checkpoints/model_27000/config.json[0m
[32m[2024-06-19 13:50:54,321] [    INFO][0m - Loading weights file model_checkpoints/model_27000/model_state.pdparams[0m
[32m[2024-06-19 13:50:56,336] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0619 13:50:56.365024 232821 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 13:50:56.366297 232821 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-19 13:50:59,679] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_27000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-19 13:50:59,679] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_27000 and are newly initialized: ['encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.17.norm1.bias', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.1.norm1.bias', 'embeddings.layer_norm.bias', 'encoder.layers.9.linear2.weight', 'pooler.dense.weight', 'encoder.layers.6.linear1.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.16.linear2.weight', 'encoder.layers.19.linear1.weight', 'encoder.layers.6.norm1.weight', 'encoder.layers.14.norm1.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.14.linear2.weight', 'encoder.layers.3.norm1.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.17.norm2.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layers.5.norm2.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.3.linear1.bias', 'encoder.layers.5.linear2.weight', 'encoder.layers.11.linear2.weight', 'encoder.layers.8.linear1.weight', 'encoder.layers.15.norm1.weight', 'encoder.layers.0.linear1.weight', 'encoder.layers.3.linear1.weight', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.6.linear1.weight', 'encoder.layers.13.norm2.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.13.norm2.weight', 'encoder.layers.19.norm1.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.11.norm1.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.14.linear2.bias', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.15.linear2.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.9.norm2.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'embeddings.word_embeddings.weight', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.3.linear2.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.4.norm2.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.4.linear2.weight', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.15.norm1.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.14.linear1.weight', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.10.norm2.bias', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.18.norm1.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.5.linear1.bias', 'encoder.layers.19.linear2.bias', 'encoder.layers.6.linear2.bias', 'encoder.layers.10.norm2.weight', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.10.linear1.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.16.norm1.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.8.linear1.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.12.linear1.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.16.norm2.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.14.norm1.weight', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.12.norm2.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.12.norm1.weight', 'encoder.layers.18.linear1.bias', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.16.linear2.bias', 'encoder.layers.1.linear2.bias', 'encoder.layers.10.norm1.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.10.linear2.weight', 'encoder.layers.11.linear1.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.16.norm1.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.2.norm2.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.14.linear1.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.2.linear2.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.4.norm2.bias', 'encoder.layers.4.linear1.weight', 'encoder.layers.7.norm2.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.6.linear2.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.13.linear1.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.7.norm1.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.16.linear1.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.17.linear2.bias', 'encoder.layers.16.linear1.bias', 'pooler.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.19.norm2.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.12.linear2.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.1.norm1.weight', 'embeddings.task_type_embeddings.weight', 'encoder.layers.12.linear2.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.17.linear2.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.9.norm2.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.10.linear2.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.15.norm2.weight', 'encoder.layers.2.linear1.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.16.norm2.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.9.norm1.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.14.norm2.weight', 'encoder.layers.19.norm2.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.3.norm2.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.10.norm1.weight', 'encoder.layers.11.norm2.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.13.linear2.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.7.linear2.bias', 'encoder.layers.11.norm1.bias', 'encoder.layers.5.norm2.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.11.norm2.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.17.norm1.weight', 'encoder.layers.15.linear2.weight', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.18.linear2.bias', 'encoder.layers.2.linear1.weight', 'encoder.layers.5.linear1.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.17.norm2.bias', 'embeddings.layer_norm.weight', 'encoder.layers.19.linear2.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.18.norm2.weight', 'encoder.layers.17.linear1.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.8.norm1.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.10.linear1.weight', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.0.norm1.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.3.norm2.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.14.norm2.bias', 'encoder.layers.4.norm1.bias', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.9.linear2.bias', 'encoder.layers.7.linear1.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.13.linear2.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.5.linear2.bias', 'encoder.layers.13.linear1.weight', 'encoder.layers.7.linear2.weight', 'encoder.layers.5.norm1.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.3.linear2.bias', 'encoder.layers.1.linear1.bias', 'encoder.layers.11.linear1.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.8.linear2.bias', 'encoder.layers.0.linear1.bias', 'encoder.layers.6.norm2.bias', 'encoder.layers.12.norm1.bias', 'encoder.layers.18.norm2.bias', 'encoder.layers.2.linear2.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.19.linear1.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.4.linear2.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.linear1.weight', 'encoder.layers.15.linear1.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.9.linear1.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.8.linear2.weight', 'encoder.layers.11.linear2.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.7.norm2.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.12.norm2.weight', 'encoder.layers.1.linear2.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.18.linear2.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.19.norm1.bias', 'encoder.layers.7.linear1.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.17.linear1.weight', 'encoder.layers.13.norm1.bias', 'encoder.layers.13.norm1.weight', 'encoder.layers.0.linear2.bias', 'encoder.layers.6.norm1.bias', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.0.linear2.weight', 'encoder.layers.3.norm1.weight', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.2.norm2.bias', 'encoder.layers.18.linear1.weight', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.2.norm1.bias', 'encoder.layers.9.linear1.bias', 'encoder.layers.7.norm1.bias', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
[32m[2024-06-19 13:51:01,727] [    INFO][0m - Loaded parameters from model_checkpoints/model_27000/model_state.pdparams[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
[32m[2024-06-19 13:54:54,085] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 13:54:54,117] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 13:54:54,117] [    INFO][0m - Loading configuration file model_checkpoints/model_27000/config.json[0m
[32m[2024-06-19 13:54:54,118] [    INFO][0m - Loading weights file model_checkpoints/model_27000/model_state.pdparams[0m
[32m[2024-06-19 13:54:56,206] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0619 13:54:56.213605 239840 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 13:54:56.215032 239840 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-19 13:54:59,603] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_27000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-19 13:54:59,603] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_27000 and are newly initialized: ['pooler.dense.bias', 'embeddings.layer_norm.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.16.self_attn.q_proj.weight', 'embeddings.word_embeddings.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.9.linear1.weight', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.3.linear2.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.6.linear1.weight', 'encoder.layers.9.norm2.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.7.norm1.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.15.linear1.weight', 'embeddings.layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.15.linear1.bias', 'encoder.layers.5.norm1.bias', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.2.linear1.weight', 'encoder.layers.3.norm1.weight', 'encoder.layers.9.linear2.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.10.linear1.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.11.linear1.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.15.norm1.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.10.linear1.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.15.norm2.bias', 'encoder.layers.17.linear2.weight', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.8.norm2.bias', 'encoder.layers.11.norm1.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.7.linear2.bias', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.10.linear2.weight', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.12.norm2.weight', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.17.norm2.weight', 'encoder.layers.18.norm1.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.2.norm2.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.17.linear1.weight', 'encoder.layers.1.linear1.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.19.linear2.bias', 'encoder.layers.18.norm2.weight', 'encoder.layers.13.norm2.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.16.norm1.bias', 'encoder.layers.5.linear1.bias', 'encoder.layers.10.linear2.bias', 'encoder.layers.8.norm1.weight', 'encoder.layers.0.linear1.weight', 'encoder.layers.19.linear1.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.19.linear2.weight', 'encoder.layers.3.norm2.bias', 'encoder.layers.13.norm2.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.17.norm2.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.19.norm1.weight', 'encoder.layers.11.norm2.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.7.norm2.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.13.linear1.bias', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.7.linear2.weight', 'encoder.layers.16.linear1.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.4.linear1.weight', 'encoder.layers.3.norm2.weight', 'encoder.layers.17.linear2.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.9.linear2.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.6.norm1.bias', 'encoder.layers.12.norm2.bias', 'encoder.layers.17.norm1.weight', 'encoder.layers.0.norm1.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.3.linear2.weight', 'encoder.layers.7.norm2.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.0.linear2.weight', 'encoder.layers.6.norm1.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.13.linear2.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.14.linear2.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.7.linear1.weight', 'encoder.layers.1.linear2.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.7.linear1.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.16.linear2.weight', 'encoder.layers.17.linear1.bias', 'encoder.layers.12.linear2.weight', 'encoder.layers.8.linear2.bias', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.3.norm1.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.6.linear2.bias', 'encoder.layers.12.linear1.bias', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.14.norm1.bias', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.6.norm2.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.14.norm2.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.5.linear1.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.12.norm1.weight', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.15.linear2.bias', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.4.norm2.weight', 'encoder.layers.14.linear1.bias', 'encoder.layers.10.norm1.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'embeddings.task_type_embeddings.weight', 'encoder.layers.19.norm1.bias', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.8.linear2.weight', 'encoder.layers.12.linear2.bias', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.16.linear1.bias', 'encoder.layers.4.linear2.bias', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.11.linear2.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.2.linear2.weight', 'encoder.layers.2.linear2.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.15.norm1.weight', 'encoder.layers.16.norm1.weight', 'encoder.layers.1.linear1.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.1.linear2.bias', 'encoder.layers.9.linear1.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.9.norm2.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.13.linear1.weight', 'encoder.layers.4.norm1.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.6.linear2.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.13.norm1.bias', 'embeddings.position_embeddings.weight', 'encoder.layers.10.norm2.weight', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.18.linear1.bias', 'encoder.layers.11.norm1.weight', 'encoder.layers.1.norm2.weight', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.13.linear2.weight', 'encoder.layers.8.linear1.weight', 'encoder.layers.0.norm2.weight', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.5.linear2.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.19.norm2.bias', 'encoder.layers.4.norm1.bias', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.0.linear2.bias', 'encoder.layers.8.linear1.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.11.norm2.bias', 'encoder.layers.10.norm1.bias', 'encoder.layers.12.norm1.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.7.norm1.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.8.norm1.bias', 'encoder.layers.18.norm2.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.16.norm2.weight', 'encoder.layers.19.linear1.weight', 'encoder.layers.14.norm1.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.11.linear1.weight', 'encoder.layers.1.norm1.bias', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.11.linear2.bias', 'encoder.layers.14.linear2.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.6.linear1.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.17.norm1.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.16.norm2.bias', 'encoder.layers.18.linear1.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layers.4.linear2.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.18.linear2.weight', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.9.norm1.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.19.norm2.weight', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.15.linear2.weight', 'encoder.layers.13.norm1.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.18.linear2.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.16.linear2.bias', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.5.linear2.weight', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.14.linear1.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.2.linear1.bias', 'encoder.layers.5.norm2.bias', 'encoder.layers.14.norm2.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.15.norm2.weight', 'encoder.layers.4.norm2.bias', 'pooler.dense.weight', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
[32m[2024-06-19 13:55:01,655] [    INFO][0m - Loaded parameters from model_checkpoints/model_27000/model_state.pdparams[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 29, in <module>
    from ann_util import build_index
  File "/home/aistudio/work/model/ann_util.py", line 20, in <module>
    import hnswlib
ModuleNotFoundError: No module named 'hnswlib'
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 29, in <module>
    from ann_util import build_index
  File "/home/aistudio/work/model/ann_util.py", line 20, in <module>
    import hnswlib
ModuleNotFoundError: No module named 'hnswlib'
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 29, in <module>
    from ann_util import build_index
  File "/home/aistudio/work/model/ann_util.py", line 20, in <module>
    import hnswlib
ModuleNotFoundError: No module named 'hnswlib'
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 29, in <module>
    from ann_util import build_index
  File "/home/aistudio/work/model/ann_util.py", line 20, in <module>
    import hnswlib
ModuleNotFoundError: No module named 'hnswlib'
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 29, in <module>
    from ann_util import build_index
  File "/home/aistudio/work/model/ann_util.py", line 20, in <module>
    import hnswlib
ModuleNotFoundError: No module named 'hnswlib'
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 29, in <module>
    from ann_util import build_index
  File "/home/aistudio/work/model/ann_util.py", line 20, in <module>
    import hnswlib
ModuleNotFoundError: No module named 'hnswlib'
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-19 15:31:06,417] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 15:31:06,444] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 15:31:06,444] [    INFO][0m - Loading configuration file model_checkpoints/model_27000/config.json[0m
[32m[2024-06-19 15:31:06,445] [    INFO][0m - Loading weights file model_checkpoints/model_27000/model_state.pdparams[0m
[32m[2024-06-19 15:31:10,657] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0619 15:31:10.685968 13713 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 15:31:10.687342 13713 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-19 15:31:20,513] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_27000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-19 15:31:20,514] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_27000 and are newly initialized: ['encoder.layers.5.linear2.weight', 'encoder.layers.2.linear2.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.9.linear2.weight', 'encoder.layers.5.linear2.bias', 'pooler.dense.weight', 'encoder.layers.18.norm2.bias', 'encoder.layers.6.linear2.bias', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.6.linear1.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.11.norm2.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.10.linear2.weight', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.18.linear1.weight', 'encoder.layers.9.linear1.bias', 'encoder.layers.12.linear2.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.6.linear2.weight', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.9.norm1.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.8.norm2.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.15.norm1.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.17.norm1.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.linear1.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.19.linear2.weight', 'encoder.layers.9.linear1.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.13.norm1.bias', 'encoder.layers.11.linear2.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.8.norm1.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.19.linear2.bias', 'encoder.layers.7.linear2.bias', 'encoder.layers.3.norm1.bias', 'encoder.layers.2.norm1.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.16.norm2.weight', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.14.linear1.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.18.linear2.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.3.linear2.bias', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.19.norm1.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.8.linear1.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.1.linear2.weight', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.7.linear1.weight', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.12.linear2.weight', 'encoder.layers.17.norm1.bias', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.10.linear2.bias', 'encoder.layers.18.linear1.bias', 'encoder.layers.14.linear2.weight', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.17.linear2.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.17.linear2.bias', 'encoder.layers.7.norm2.bias', 'encoder.layers.15.norm1.bias', 'encoder.layers.2.linear1.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.6.norm2.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.14.norm1.weight', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.2.linear2.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.1.linear1.weight', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.14.linear1.weight', 'encoder.layers.11.norm1.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.7.norm1.weight', 'encoder.layers.10.linear1.weight', 'embeddings.layer_norm.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.10.norm1.bias', 'encoder.layers.0.linear2.bias', 'encoder.layers.19.norm2.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.7.linear2.weight', 'encoder.layers.7.linear1.bias', 'encoder.layers.6.norm1.bias', 'encoder.layers.17.norm2.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.19.linear1.bias', 'encoder.layers.5.norm2.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.11.norm1.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.6.norm1.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.4.linear1.bias', 'embeddings.task_type_embeddings.weight', 'encoder.layers.2.norm1.weight', 'encoder.layers.12.norm1.bias', 'encoder.layers.13.linear1.bias', 'encoder.layers.15.linear2.weight', 'encoder.layers.5.norm2.weight', 'encoder.layers.14.linear2.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.14.norm2.bias', 'encoder.layers.15.linear1.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.16.norm1.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.19.norm2.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.2.norm2.bias', 'encoder.layers.16.norm2.bias', 'encoder.layers.13.norm1.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.11.linear2.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.9.norm2.bias', 'encoder.layers.10.norm1.weight', 'encoder.layers.3.linear2.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.16.linear1.bias', 'encoder.layers.5.linear1.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.11.linear1.weight', 'encoder.layers.18.norm1.bias', 'encoder.layers.13.norm2.weight', 'encoder.layers.4.linear2.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layers.19.norm1.weight', 'embeddings.layer_norm.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'embeddings.position_embeddings.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.17.linear1.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.0.linear1.weight', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.1.norm1.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.10.norm2.weight', 'encoder.layers.12.norm2.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.13.linear1.weight', 'encoder.layers.13.linear2.bias', 'encoder.layers.2.linear1.bias', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.11.norm2.weight', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.19.linear1.weight', 'encoder.layers.11.linear1.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.9.linear2.bias', 'encoder.layers.12.norm1.weight', 'encoder.layers.16.linear1.weight', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.16.linear2.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.13.linear2.weight', 'embeddings.word_embeddings.weight', 'encoder.layers.16.norm1.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.8.linear2.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.7.norm2.weight', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.9.norm2.weight', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.4.linear2.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.15.norm2.weight', 'encoder.layers.3.norm1.weight', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.7.norm1.bias', 'encoder.layers.4.norm1.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.14.norm2.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.13.norm2.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.8.linear2.weight', 'encoder.layers.18.linear2.bias', 'encoder.layers.16.linear2.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.17.norm2.weight', 'encoder.layers.0.linear2.weight', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.17.linear1.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.12.linear1.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.15.linear2.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.18.norm2.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.1.linear1.bias', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.4.linear1.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.1.linear2.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.2.norm2.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.10.linear1.bias', 'encoder.layers.12.norm2.bias', 'encoder.layers.14.norm1.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.0.norm2.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.8.linear1.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.4.norm2.bias', 'encoder.layers.4.norm2.weight', 'pooler.dense.bias', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.3.norm2.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 105, in <module>
    final_index = pickle.load(wfile)
ModuleNotFoundError: No module named 'hnswlib'
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
W0619 15:32:56.496042 15482 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 15:32:56.497457 15482 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 111, in <module>
    text_list = gen_text_file_from_json(args.input_json_file)
  File "/home/aistudio/work/model/data.py", line 155, in gen_text_file_from_json
    with open(input_json_file, "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: 'dev.json'
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
W0619 15:36:35.642931 21411 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 15:36:35.644284 21411 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 111, in <module>
    text_list = gen_text_file_from_json(args.input_json_file)
  File "/home/aistudio/work/model/data.py", line 155, in gen_text_file_from_json
    with open(input_json_file, "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: 'dev.json'
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
W0619 15:42:31.700060 30715 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 15:42:31.701372 30715 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
W0619 15:56:05.816473 52626 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 15:56:05.817867 52626 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/bin/python: can't open file '/home/aistudio/work/model/ignore': [Errno 2] No such file or directory
/opt/conda/envs/python35-paddle120-env/bin/python: can't open file '/home/aistudio/work/model/ignore': [Errno 2] No such file or directory
/opt/conda/envs/python35-paddle120-env/bin/python: can't open file '/home/aistudio/work/model/ignore': [Errno 2] No such file or directory
/opt/conda/envs/python35-paddle120-env/bin/python: can't open file '/home/aistudio/work/model/ignore': [Errno 2] No such file or directory
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-19 16:04:51,486] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 16:04:51,598] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 16:04:51,685] [    INFO][0m - Loading configuration file model_checkpoints/model_27000/config.json[0m
[32m[2024-06-19 16:04:51,688] [    INFO][0m - Loading weights file model_checkpoints/model_27000/model_state.pdparams[0m
[32m[2024-06-19 16:04:56,576] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0619 16:04:56.580749 66757 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 16:04:56.586434 66757 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-19 16:05:02,716] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_27000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-19 16:05:02,716] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_27000 and are newly initialized: ['encoder.layers.12.norm1.bias', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.2.linear2.weight', 'encoder.layers.15.linear1.bias', 'encoder.layers.10.norm1.weight', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.9.linear1.weight', 'encoder.layers.12.linear2.weight', 'encoder.layers.17.linear2.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.14.linear1.weight', 'encoder.layers.4.linear2.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.1.norm1.bias', 'encoder.layers.0.norm1.bias', 'encoder.layers.12.norm2.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.13.linear1.bias', 'encoder.layers.18.norm2.weight', 'encoder.layers.6.linear1.weight', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.0.linear2.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.14.linear2.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.19.linear2.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.14.norm2.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.3.norm2.weight', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.16.linear1.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.13.norm2.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.19.norm2.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.12.linear1.bias', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.18.linear1.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.11.norm2.weight', 'encoder.layers.4.norm1.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'embeddings.word_embeddings.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.3.linear2.bias', 'encoder.layers.3.norm1.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.5.linear2.weight', 'encoder.layers.16.linear2.weight', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.3.linear1.bias', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.3.norm2.bias', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.0.linear1.bias', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.1.linear1.bias', 'encoder.layers.8.linear2.weight', 'encoder.layers.7.norm2.bias', 'encoder.layers.8.linear1.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.9.norm2.weight', 'encoder.layers.8.norm1.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.6.norm1.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'pooler.dense.weight', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.7.norm1.weight', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.19.norm2.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.4.norm2.bias', 'encoder.layers.9.norm1.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.6.norm2.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.18.linear2.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.3.linear2.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.5.norm2.bias', 'encoder.layers.7.norm1.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.11.norm1.weight', 'encoder.layers.12.norm2.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.7.linear2.bias', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.10.norm1.bias', 'encoder.layers.5.linear1.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'embeddings.position_embeddings.weight', 'encoder.layers.0.linear2.bias', 'encoder.layers.16.norm1.bias', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.6.linear2.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.8.linear2.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.1.norm1.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.11.linear1.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.15.linear2.weight', 'encoder.layers.12.linear2.bias', 'encoder.layers.14.linear1.bias', 'encoder.layers.13.linear2.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.13.norm1.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.7.linear2.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.14.linear2.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.19.norm1.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.16.linear2.bias', 'encoder.layers.19.linear1.weight', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.19.linear1.bias', 'encoder.layers.10.linear1.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.17.linear2.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.11.linear2.bias', 'encoder.layers.10.linear2.weight', 'encoder.layers.11.norm2.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.15.linear2.bias', 'encoder.layers.7.linear1.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.13.norm2.bias', 'encoder.layers.17.norm1.bias', 'encoder.layers.18.norm1.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.2.norm2.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.0.linear1.weight', 'encoder.layers.18.linear2.weight', 'encoder.layers.17.norm1.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.10.norm2.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.9.norm2.bias', 'encoder.layers.13.norm1.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.16.linear1.bias', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.18.linear1.weight', 'embeddings.layer_norm.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.2.linear1.weight', 'encoder.layers.2.norm1.weight', 'encoder.layers.3.linear1.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'embeddings.layer_norm.bias', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.17.norm2.bias', 'encoder.layers.14.norm2.bias', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.11.linear1.weight', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.16.norm1.weight', 'encoder.layers.15.norm1.bias', 'encoder.layers.13.linear1.weight', 'encoder.layers.18.norm2.bias', 'pooler.dense.bias', 'encoder.layers.9.linear2.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.7.linear1.weight', 'encoder.layers.14.norm1.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.5.norm1.bias', 'encoder.layers.14.norm1.weight', 'encoder.layers.9.linear2.weight', 'encoder.layers.4.linear1.weight', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.17.linear1.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.10.linear1.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.v_proj.bias', 'embeddings.task_type_embeddings.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.6.norm1.bias', 'encoder.layers.15.norm2.weight', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.1.linear2.bias', 'encoder.layers.4.norm1.bias', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.16.norm2.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.7.norm2.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.1.linear1.weight', 'encoder.layers.6.linear1.bias', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.16.norm2.bias', 'encoder.layers.4.norm2.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.17.linear1.weight', 'encoder.layers.19.norm1.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.13.linear2.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.15.norm1.weight', 'encoder.layers.5.linear2.bias', 'encoder.layers.6.linear2.weight', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.3.norm1.bias', 'encoder.layers.2.linear1.bias', 'encoder.layers.12.norm1.weight', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.11.linear2.weight', 'encoder.layers.10.linear2.bias', 'encoder.layers.1.linear2.weight', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.11.norm1.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.8.linear1.bias', 'encoder.layers.9.linear1.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.4.linear2.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.17.norm2.weight', 'encoder.layers.19.linear2.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.linear2.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.1.norm2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-19 16:11:21,761] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 16:11:21,796] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 16:11:21,796] [    INFO][0m - Loading configuration file model_checkpoints/model_27000/config.json[0m
[32m[2024-06-19 16:11:21,797] [    INFO][0m - Loading weights file model_checkpoints/model_27000/model_state.pdparams[0m
[32m[2024-06-19 16:11:24,753] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0619 16:11:24.757570 77399 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 16:11:24.759034 77399 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-19 16:11:31,135] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_27000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-19 16:11:31,135] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_27000 and are newly initialized: ['encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.19.linear1.weight', 'encoder.layers.18.norm2.bias', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.9.norm2.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.16.linear2.bias', 'encoder.layers.13.linear2.weight', 'encoder.layers.16.norm2.bias', 'encoder.layers.19.norm1.bias', 'encoder.layers.11.norm1.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.16.norm1.weight', 'encoder.layers.2.linear1.weight', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.8.norm1.weight', 'encoder.layers.9.linear2.weight', 'encoder.layers.11.linear1.bias', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.10.norm2.bias', 'encoder.layers.11.norm2.bias', 'encoder.layers.17.linear1.weight', 'encoder.layers.14.norm2.weight', 'encoder.layers.5.linear1.weight', 'encoder.layers.12.norm2.bias', 'encoder.layers.16.norm2.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'embeddings.layer_norm.bias', 'embeddings.layer_norm.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'pooler.dense.weight', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.14.norm1.weight', 'encoder.layers.9.linear1.bias', 'pooler.dense.bias', 'encoder.layers.15.linear1.bias', 'encoder.layers.11.norm1.weight', 'encoder.layers.7.linear1.bias', 'encoder.layers.14.norm1.bias', 'encoder.layers.11.linear2.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.1.norm2.bias', 'encoder.layers.8.norm1.bias', 'encoder.layers.15.linear2.bias', 'encoder.layers.2.linear2.weight', 'encoder.layers.18.linear2.weight', 'encoder.layers.3.norm1.bias', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.2.norm1.weight', 'encoder.layers.8.linear1.weight', 'encoder.layers.16.linear1.bias', 'encoder.layers.13.linear1.bias', 'encoder.layers.19.linear1.bias', 'embeddings.word_embeddings.weight', 'encoder.layers.10.linear1.bias', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.5.norm2.bias', 'encoder.layers.2.linear2.bias', 'encoder.layers.7.linear1.weight', 'encoder.layers.10.norm1.weight', 'encoder.layers.0.norm1.weight', 'encoder.layers.13.linear1.weight', 'encoder.layers.19.norm2.bias', 'encoder.layers.17.norm1.weight', 'encoder.layers.6.linear2.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.6.linear1.weight', 'encoder.layers.13.norm1.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.13.norm2.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.6.norm1.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.19.norm2.weight', 'encoder.layers.12.linear1.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.3.norm1.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.1.linear2.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.10.norm2.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.11.linear2.weight', 'encoder.layers.3.linear2.weight', 'encoder.layers.0.norm2.weight', 'encoder.layers.10.linear2.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.13.linear2.bias', 'embeddings.position_embeddings.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.2.linear1.bias', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.9.norm2.weight', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.1.norm2.weight', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.9.norm1.weight', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.14.norm2.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.6.linear2.bias', 'encoder.layers.11.norm2.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.17.norm2.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.13.norm1.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.14.linear2.bias', 'encoder.layers.4.norm2.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.12.linear2.weight', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.16.norm1.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.18.norm2.weight', 'encoder.layers.7.norm2.bias', 'encoder.layers.1.linear1.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.14.linear1.bias', 'encoder.layers.3.linear1.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.13.norm2.bias', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.18.linear1.bias', 'encoder.layers.5.linear2.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.6.norm2.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.12.linear2.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.14.linear1.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.19.norm1.weight', 'encoder.layers.5.linear2.bias', 'encoder.layers.6.linear1.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.10.linear1.weight', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.15.norm1.weight', 'encoder.layers.18.linear1.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.18.norm1.bias', 'encoder.layers.4.linear2.bias', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.1.linear1.bias', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.19.linear2.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layers.3.linear1.weight', 'encoder.layers.17.linear2.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.4.linear2.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.8.linear2.weight', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.15.norm1.bias', 'encoder.layers.3.linear2.bias', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.12.norm2.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.8.linear2.bias', 'encoder.layers.0.linear1.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.0.linear2.bias', 'encoder.layers.1.linear2.weight', 'encoder.layers.15.norm2.weight', 'encoder.layers.10.linear2.bias', 'encoder.layers.16.linear2.weight', 'encoder.layers.12.norm1.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.1.norm1.bias', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.11.linear1.weight', 'encoder.layers.14.linear2.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.4.norm2.weight', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.10.norm1.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.17.norm1.bias', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.16.linear1.weight', 'encoder.layers.1.norm1.weight', 'encoder.layers.7.linear2.bias', 'encoder.layers.7.norm1.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.7.norm1.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.7.linear2.weight', 'encoder.layers.9.linear2.bias', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.4.linear1.weight', 'encoder.layers.4.norm1.bias', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.17.norm2.weight', 'embeddings.task_type_embeddings.weight', 'encoder.layers.17.linear1.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.19.linear2.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.15.linear2.weight', 'encoder.layers.12.norm1.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.6.norm1.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.17.linear2.weight', 'encoder.layers.8.linear1.bias', 'encoder.layers.9.linear1.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.18.linear2.bias', 'encoder.layers.7.norm2.weight', 'encoder.layers.3.norm2.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-19 16:20:44,566] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 16:20:44,600] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 16:20:44,600] [    INFO][0m - Loading configuration file model_checkpoints/model_27000/config.json[0m
[32m[2024-06-19 16:20:44,601] [    INFO][0m - Loading weights file model_checkpoints/model_27000/model_state.pdparams[0m
[32m[2024-06-19 16:20:47,425] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0619 16:20:47.429179 92171 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 16:20:47.430506 92171 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-19 16:20:54,334] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_27000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-19 16:20:54,335] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_27000 and are newly initialized: ['encoder.layers.11.linear1.weight', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.7.norm2.bias', 'encoder.layers.13.norm1.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.2.linear2.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'embeddings.layer_norm.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.10.linear2.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.5.linear1.bias', 'encoder.layers.15.norm2.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.12.norm2.bias', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.14.linear1.bias', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.14.norm2.weight', 'encoder.layers.6.linear1.weight', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.1.linear1.bias', 'encoder.layers.15.norm1.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.19.linear1.bias', 'encoder.layers.14.linear2.weight', 'encoder.layers.17.linear2.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.17.norm2.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.10.norm2.weight', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.7.linear1.bias', 'encoder.layers.19.norm2.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.1.linear2.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.14.linear1.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.6.norm2.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.3.linear2.bias', 'encoder.layers.13.norm1.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.8.linear2.weight', 'encoder.layers.14.norm1.bias', 'encoder.layers.1.linear1.weight', 'encoder.layers.18.linear2.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.11.norm2.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.17.linear1.bias', 'encoder.layers.7.linear1.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.2.linear1.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.10.norm1.bias', 'encoder.layers.13.norm2.bias', 'encoder.layers.10.norm1.weight', 'encoder.layers.16.linear1.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.0.norm2.bias', 'encoder.layers.15.norm2.weight', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.17.linear1.weight', 'encoder.layers.8.norm1.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.6.linear2.bias', 'encoder.layers.9.norm2.weight', 'encoder.layers.8.linear2.bias', 'encoder.layers.18.norm2.bias', 'encoder.layers.15.linear2.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.11.linear1.bias', 'encoder.layers.1.linear2.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.7.norm1.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.1.norm1.weight', 'encoder.layers.12.linear1.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.13.linear2.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.18.linear2.weight', 'encoder.layers.17.norm1.weight', 'encoder.layers.10.linear2.weight', 'encoder.layers.1.norm1.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.10.norm2.bias', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.4.linear2.bias', 'encoder.layers.8.linear1.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.17.norm1.bias', 'encoder.layers.7.linear2.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.14.norm2.bias', 'encoder.layers.11.norm1.weight', 'encoder.layers.5.linear2.bias', 'encoder.layers.13.linear1.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.6.norm1.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.15.norm1.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.19.norm1.bias', 'encoder.layers.0.linear2.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.15.linear2.weight', 'encoder.layers.18.norm2.weight', 'encoder.layers.4.norm1.bias', 'encoder.layers.3.norm1.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.13.linear2.weight', 'encoder.layers.7.norm2.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.3.norm2.weight', 'embeddings.task_type_embeddings.weight', 'encoder.layers.4.linear1.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.2.norm2.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.2.norm1.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.9.linear2.weight', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.3.norm2.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.9.linear1.weight', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.16.norm2.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.11.norm1.bias', 'encoder.layers.12.self_attn.k_proj.bias', 'pooler.dense.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.16.norm2.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.13.norm2.weight', 'encoder.layers.5.linear2.weight', 'encoder.layers.16.linear1.weight', 'encoder.layers.10.linear1.bias', 'encoder.layers.19.linear1.weight', 'encoder.layers.9.linear1.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.18.linear1.bias', 'encoder.layers.1.norm2.bias', 'encoder.layers.2.norm1.bias', 'encoder.layers.19.linear2.weight', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.18.norm1.bias', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.13.linear1.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.5.norm2.weight', 'embeddings.word_embeddings.weight', 'encoder.layers.6.norm1.bias', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.11.linear2.bias', 'encoder.layers.9.norm1.weight', 'encoder.layers.16.norm1.bias', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.16.norm1.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.14.norm1.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.4.norm2.weight', 'encoder.layers.12.norm1.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layers.12.linear2.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.2.linear2.bias', 'encoder.layers.9.linear2.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.2.linear1.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.7.norm1.bias', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.18.linear1.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.6.linear1.bias', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.12.norm1.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.11.linear2.weight', 'encoder.layers.6.linear2.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.3.norm1.weight', 'encoder.layers.17.norm2.bias', 'encoder.layers.8.linear1.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.19.linear2.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.19.norm1.weight', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.12.linear2.bias', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.14.linear2.bias', 'encoder.layers.16.linear2.bias', 'encoder.layers.9.norm2.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.5.linear1.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'embeddings.position_embeddings.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.10.linear1.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.8.norm2.weight', 'encoder.layers.16.linear2.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.17.linear2.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.3.linear2.weight', 'encoder.layers.4.linear2.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.5.norm2.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'embeddings.layer_norm.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.15.linear1.bias', 'encoder.layers.0.norm1.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.11.norm2.weight', 'encoder.layers.19.self_attn.k_proj.bias', 'pooler.dense.bias', 'encoder.layers.12.norm2.weight', 'encoder.layers.0.linear1.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.19.norm2.weight', 'encoder.layers.4.norm1.weight', 'encoder.layers.7.linear2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-19 16:33:37,908] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 16:33:37,933] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_27000'.[0m
[32m[2024-06-19 16:33:37,934] [    INFO][0m - Loading configuration file model_checkpoints/model_27000/config.json[0m
[32m[2024-06-19 16:33:37,935] [    INFO][0m - Loading weights file model_checkpoints/model_27000/model_state.pdparams[0m
[32m[2024-06-19 16:33:40,887] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0619 16:33:40.891662 113014 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 16:33:40.893199 113014 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-19 16:33:47,139] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_27000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-19 16:33:47,139] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_27000 and are newly initialized: ['encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.9.norm2.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.19.norm2.weight', 'encoder.layers.2.linear1.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.15.norm1.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.3.linear2.weight', 'embeddings.word_embeddings.weight', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.10.linear1.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'embeddings.layer_norm.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'embeddings.position_embeddings.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.0.norm1.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.15.linear1.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.4.norm2.weight', 'encoder.layers.15.norm2.weight', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.3.linear2.bias', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.k_proj.bias', 'pooler.dense.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.17.norm2.weight', 'encoder.layers.1.norm2.weight', 'encoder.layers.6.norm1.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.16.norm1.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.19.linear1.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.4.linear2.weight', 'encoder.layers.19.linear2.bias', 'encoder.layers.16.linear2.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.13.linear2.weight', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.5.norm2.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.7.linear2.bias', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.19.norm2.bias', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.12.norm1.bias', 'encoder.layers.18.norm2.weight', 'embeddings.layer_norm.weight', 'encoder.layers.0.linear2.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.2.linear2.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.1.linear2.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.1.norm1.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.5.linear2.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.15.linear2.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.7.norm1.weight', 'encoder.layers.19.linear1.weight', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.7.norm2.bias', 'encoder.layers.11.linear2.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.6.linear2.bias', 'embeddings.task_type_embeddings.weight', 'encoder.layers.9.linear2.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.17.linear1.weight', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.8.linear1.weight', 'encoder.layers.14.linear1.bias', 'encoder.layers.12.linear1.bias', 'encoder.layers.1.linear2.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.6.norm1.bias', 'encoder.layers.17.linear2.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.7.norm2.weight', 'encoder.layers.2.linear1.bias', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.12.linear2.weight', 'encoder.layers.8.linear2.bias', 'encoder.layers.10.linear2.bias', 'encoder.layers.0.linear2.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.13.linear1.bias', 'encoder.layers.3.norm2.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.11.linear2.weight', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.9.norm2.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.14.linear2.weight', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.4.norm1.bias', 'encoder.layers.11.norm2.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.11.linear1.bias', 'encoder.layers.6.linear1.weight', 'encoder.layers.9.linear1.bias', 'encoder.layers.14.linear2.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.19.norm1.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.10.linear2.weight', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.9.linear1.weight', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.7.linear1.bias', 'encoder.layers.11.linear1.weight', 'encoder.layers.18.linear2.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.10.norm1.bias', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.7.norm1.bias', 'encoder.layers.9.linear2.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.18.linear1.bias', 'encoder.layers.19.norm1.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layers.14.norm1.weight', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.16.norm2.bias', 'encoder.layers.16.norm2.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.0.linear1.weight', 'encoder.layers.13.norm2.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.16.linear2.weight', 'encoder.layers.1.linear1.weight', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.8.norm1.weight', 'encoder.layers.4.linear1.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.12.norm2.bias', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.17.linear2.bias', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.9.norm1.bias', 'encoder.layers.16.linear1.bias', 'encoder.layers.2.linear2.bias', 'encoder.layers.6.norm2.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.12.linear1.weight', 'encoder.layers.13.norm1.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.13.norm2.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.18.norm1.bias', 'encoder.layers.5.linear2.weight', 'encoder.layers.16.norm1.bias', 'encoder.layers.18.linear2.bias', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.10.norm2.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.2.norm2.weight', 'encoder.layers.12.linear2.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.18.linear1.weight', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.18.norm2.bias', 'encoder.layers.13.linear2.bias', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.15.linear2.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.17.norm1.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.3.norm1.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.13.norm1.bias', 'encoder.layers.0.linear1.bias', 'encoder.layers.15.norm1.weight', 'encoder.layers.5.norm2.weight', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.6.linear1.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.14.norm1.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.9.norm1.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.11.norm2.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.8.norm2.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.14.norm2.bias', 'encoder.layers.7.linear2.weight', 'encoder.layers.3.norm1.bias', 'encoder.layers.19.linear2.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.6.linear2.weight', 'encoder.layers.16.linear1.weight', 'encoder.layers.7.linear1.weight', 'encoder.layers.5.norm1.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.2.norm1.weight', 'encoder.layers.14.norm2.weight', 'encoder.layers.13.linear1.weight', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.17.norm1.bias', 'encoder.layers.11.norm1.weight', 'encoder.layers.18.norm1.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.14.linear1.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.10.linear1.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.11.norm1.bias', 'encoder.layers.17.norm2.bias', 'encoder.layers.17.linear1.bias', 'encoder.layers.12.norm1.weight', 'encoder.layers.5.linear1.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.8.linear2.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.4.linear2.bias', 'pooler.dense.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.12.norm2.weight', 'encoder.layers.10.norm1.weight', 'encoder.layers.1.linear1.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.8.linear1.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.0.norm2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
[32m[2024-06-19 23:58:43,743] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-19 23:58:43,769] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-19 23:58:43,770] [    INFO][0m - Loading configuration file model_checkpoints/model_2000/config.json[0m
[32m[2024-06-19 23:58:43,770] [    INFO][0m - Loading weights file model_checkpoints/model_2000/model_state.pdparams[0m
[32m[2024-06-19 23:58:46,423] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0619 23:58:46.427513 234959 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0619 23:58:46.429181 234959 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-19 23:58:52,565] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_2000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-19 23:58:52,566] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_2000 and are newly initialized: ['encoder.layers.14.self_attn.v_proj.weight', 'pooler.dense.bias', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.11.norm2.weight', 'encoder.layers.19.norm1.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.3.norm2.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.4.norm1.bias', 'encoder.layers.12.norm2.weight', 'encoder.layers.11.linear1.bias', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.11.linear1.weight', 'encoder.layers.12.norm1.bias', 'encoder.layers.1.linear2.bias', 'encoder.layers.6.linear1.weight', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.13.norm2.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.1.linear1.weight', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.14.linear1.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.15.linear2.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.12.linear2.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.18.linear2.bias', 'encoder.layers.8.linear2.weight', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.13.norm1.weight', 'encoder.layers.1.linear1.bias', 'encoder.layers.7.linear2.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.18.norm1.bias', 'encoder.layers.16.linear2.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.2.linear1.weight', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.5.linear2.weight', 'encoder.layers.13.linear1.bias', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.4.norm2.weight', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.18.linear1.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.17.linear2.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.18.norm2.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.13.norm1.bias', 'encoder.layers.16.norm2.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.1.norm1.weight', 'encoder.layers.18.norm1.weight', 'encoder.layers.12.norm2.bias', 'encoder.layers.16.norm1.weight', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.16.linear1.bias', 'encoder.layers.17.linear1.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.18.linear2.weight', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.15.norm1.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.7.norm2.weight', 'encoder.layers.16.norm1.bias', 'encoder.layers.4.linear1.weight', 'encoder.layers.10.linear1.weight', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.17.norm1.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.3.norm1.weight', 'embeddings.position_embeddings.weight', 'encoder.layers.2.linear2.weight', 'encoder.layers.10.norm2.weight', 'encoder.layers.1.linear2.weight', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.11.linear2.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.11.norm1.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.8.linear2.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.8.linear1.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.7.norm1.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.12.linear2.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.8.norm1.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'embeddings.layer_norm.bias', 'encoder.layers.2.norm2.bias', 'encoder.layers.10.linear2.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.14.norm2.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.17.norm1.bias', 'encoder.layers.7.norm2.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.7.linear2.bias', 'encoder.layers.16.norm2.bias', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.13.linear2.weight', 'encoder.layers.5.norm2.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.19.norm1.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.14.norm1.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.0.norm1.bias', 'encoder.layers.0.linear2.bias', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.3.norm1.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.17.linear2.bias', 'encoder.layers.9.linear2.weight', 'encoder.layers.4.linear2.weight', 'encoder.layers.2.linear1.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layers.11.norm1.weight', 'encoder.layers.17.linear1.bias', 'encoder.layers.15.norm1.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.9.linear1.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.19.linear1.bias', 'encoder.layers.14.linear2.weight', 'pooler.dense.weight', 'encoder.layers.15.linear1.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.9.norm2.bias', 'encoder.layers.4.linear1.bias', 'encoder.layers.19.linear1.weight', 'encoder.layers.0.linear1.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.14.norm1.bias', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.9.norm2.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.3.linear2.weight', 'encoder.layers.10.linear1.bias', 'encoder.layers.3.linear2.bias', 'encoder.layers.6.linear2.bias', 'encoder.layers.1.norm2.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.9.linear1.bias', 'encoder.layers.10.linear2.bias', 'encoder.layers.13.linear2.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.6.linear1.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.11.linear2.bias', 'encoder.layers.13.norm2.bias', 'encoder.layers.6.norm1.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.19.linear2.weight', 'embeddings.word_embeddings.weight', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.6.norm2.bias', 'encoder.layers.1.norm1.bias', 'encoder.layers.18.norm2.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.0.norm2.weight', 'encoder.layers.16.linear1.weight', 'encoder.layers.6.linear2.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.14.linear1.bias', 'encoder.layers.19.norm2.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.18.linear1.weight', 'encoder.layers.12.linear1.bias', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.7.norm1.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.13.linear1.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.17.norm2.weight', 'encoder.layers.10.norm1.weight', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.15.linear2.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.4.linear2.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.8.linear1.bias', 'embeddings.task_type_embeddings.weight', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.5.linear2.bias', 'encoder.layers.14.norm2.bias', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.10.norm1.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.16.linear2.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.4.norm1.weight', 'embeddings.layer_norm.weight', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.12.norm1.weight', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.7.linear1.weight', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.2.linear2.bias', 'encoder.layers.5.linear1.weight', 'encoder.layers.8.norm2.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.15.norm2.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.14.linear2.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.11.norm2.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.19.norm2.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.5.norm2.bias', 'encoder.layers.9.linear2.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.17.norm2.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.6.norm1.bias', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.19.linear2.bias', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.7.linear1.bias', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.9.norm1.weight', 'encoder.layers.0.self_attn.k_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
10
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
180
190
200
210
220
230
240
250
260
270
[32m[2024-06-20 00:24:21,549] [    INFO][0m - start build index..........[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-20 08:56:18,695] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 08:56:18,721] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 08:56:18,721] [    INFO][0m - Loading configuration file model_checkpoints/model_2000/config.json[0m
[32m[2024-06-20 08:56:18,722] [    INFO][0m - Loading weights file model_checkpoints/model_2000/model_state.pdparams[0m
[32m[2024-06-20 08:56:21,357] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0620 08:56:21.360646  4927 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0620 08:56:21.361999  4927 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-20 08:56:26,708] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_2000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-20 08:56:26,708] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_2000 and are newly initialized: ['encoder.layers.5.linear1.weight', 'encoder.layers.9.norm2.bias', 'encoder.layers.6.norm1.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.2.linear1.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.0.norm1.bias', 'pooler.dense.weight', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.19.linear1.bias', 'encoder.layers.11.linear2.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.16.norm1.weight', 'encoder.layers.1.linear2.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.1.linear2.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.8.norm1.weight', 'encoder.layers.3.linear2.bias', 'encoder.layers.10.norm2.weight', 'encoder.layers.12.linear2.weight', 'encoder.layers.5.linear2.weight', 'encoder.layers.7.norm1.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.8.linear1.weight', 'encoder.layers.12.linear2.bias', 'encoder.layers.2.linear2.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.norm1.weight', 'encoder.layers.15.linear2.weight', 'encoder.layers.4.linear2.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.17.norm2.bias', 'encoder.layers.19.linear2.bias', 'encoder.layers.5.norm1.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.17.linear1.bias', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.10.norm2.bias', 'encoder.layers.8.linear2.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.13.linear2.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.13.norm2.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.19.norm1.bias', 'encoder.layers.2.linear2.weight', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.0.norm2.weight', 'encoder.layers.15.linear1.bias', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.10.linear2.bias', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.8.linear2.weight', 'encoder.layers.11.linear2.weight', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.12.norm1.weight', 'encoder.layers.16.linear1.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.6.norm1.weight', 'encoder.layers.7.norm1.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.3.linear2.weight', 'encoder.layers.16.linear2.bias', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.17.norm1.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.18.norm1.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layers.16.norm2.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.17.linear1.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.18.linear2.weight', 'encoder.layers.10.norm1.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.10.linear2.weight', 'encoder.layers.10.linear1.bias', 'encoder.layers.14.norm2.weight', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.14.linear2.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.3.norm1.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.7.linear1.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.3.norm2.bias', 'encoder.layers.11.norm2.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.14.linear1.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.7.linear2.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.6.norm2.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.14.linear1.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.9.linear2.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.2.linear1.bias', 'encoder.layers.11.linear1.bias', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.13.linear1.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.12.linear1.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.6.linear2.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.6.linear1.bias', 'encoder.layers.6.linear2.weight', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.4.linear2.weight', 'encoder.layers.9.linear1.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.1.norm1.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.13.norm1.weight', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.11.norm1.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.4.linear1.weight', 'encoder.layers.4.linear1.bias', 'pooler.dense.bias', 'encoder.layers.18.linear2.bias', 'encoder.layers.9.linear1.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.5.norm1.weight', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.13.linear2.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.14.norm1.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.10.norm1.weight', 'embeddings.position_embeddings.weight', 'encoder.layers.11.linear1.weight', 'encoder.layers.13.norm2.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.17.norm2.weight', 'encoder.layers.13.norm1.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.9.linear2.weight', 'encoder.layers.3.norm1.bias', 'encoder.layers.19.linear2.weight', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.9.norm1.weight', 'encoder.layers.18.linear1.bias', 'encoder.layers.16.linear2.weight', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.1.linear1.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.15.norm1.weight', 'embeddings.layer_norm.weight', 'encoder.layers.11.norm1.bias', 'embeddings.layer_norm.bias', 'encoder.layers.15.norm2.bias', 'encoder.layers.14.norm2.bias', 'encoder.layers.0.linear1.weight', 'embeddings.task_type_embeddings.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.17.linear2.bias', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.19.norm1.weight', 'encoder.layers.15.norm2.weight', 'encoder.layers.14.linear2.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.8.linear1.bias', 'encoder.layers.12.norm2.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.14.norm1.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.11.norm2.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.19.norm2.bias', 'encoder.layers.10.linear1.weight', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.17.norm1.weight', 'encoder.layers.4.norm1.bias', 'embeddings.word_embeddings.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.7.linear2.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.16.linear1.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.12.norm1.bias', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.19.linear1.weight', 'encoder.layers.6.norm2.bias', 'encoder.layers.15.norm1.bias', 'encoder.layers.16.norm2.bias', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.1.linear1.bias', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.9.norm2.weight', 'encoder.layers.18.norm2.weight', 'encoder.layers.7.norm2.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.4.norm2.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.7.linear1.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.13.linear1.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.18.norm2.bias', 'encoder.layers.4.norm2.bias', 'encoder.layers.6.linear1.weight', 'encoder.layers.16.norm1.bias', 'encoder.layers.12.norm2.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.0.linear2.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.5.norm2.bias', 'encoder.layers.17.linear2.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.7.norm2.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.15.linear2.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.19.norm2.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.5.linear2.bias', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.18.linear1.weight', 'encoder.layers.18.norm1.weight', 'encoder.layers.12.linear1.bias', 'encoder.layers.9.norm1.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 105, in <module>
    final_index = pickle.load(wfile)
ModuleNotFoundError: No module named 'hnswlib'
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-20 09:05:09,629] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 09:05:09,824] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 09:05:09,824] [    INFO][0m - Loading configuration file model_checkpoints/model_2000/config.json[0m
[32m[2024-06-20 09:05:09,826] [    INFO][0m - Loading weights file model_checkpoints/model_2000/model_state.pdparams[0m
[32m[2024-06-20 09:05:20,031] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0620 09:05:20.035243 16255 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0620 09:05:20.036588 16255 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-20 09:05:35,041] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_2000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-20 09:05:35,042] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_2000 and are newly initialized: ['encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.19.norm2.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.7.norm2.weight', 'encoder.layers.5.norm2.bias', 'encoder.layers.9.linear2.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.2.norm2.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.0.norm1.bias', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.14.linear1.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.9.linear1.bias', 'encoder.layers.8.linear2.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.9.norm2.weight', 'encoder.layers.9.linear1.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.17.norm1.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.14.norm2.bias', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.18.norm1.bias', 'encoder.layers.16.linear2.weight', 'encoder.layers.9.norm2.bias', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.7.linear2.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.18.linear1.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.10.norm2.weight', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.17.linear1.weight', 'encoder.layers.12.norm1.bias', 'encoder.layers.8.linear2.bias', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.15.linear2.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.4.linear2.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.11.norm1.bias', 'encoder.layers.11.linear1.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'pooler.dense.weight', 'encoder.layers.14.norm1.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.4.linear1.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.11.linear2.weight', 'encoder.layers.12.norm1.weight', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.0.linear1.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.19.linear1.weight', 'encoder.layers.8.norm1.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.11.linear1.weight', 'embeddings.position_embeddings.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.14.linear2.bias', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.19.norm2.bias', 'encoder.layers.16.linear1.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.3.norm1.bias', 'encoder.layers.14.linear2.weight', 'encoder.layers.11.norm1.weight', 'encoder.layers.4.linear2.weight', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.norm1.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.0.linear2.bias', 'pooler.dense.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.12.linear2.bias', 'encoder.layers.18.norm2.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.14.norm1.bias', 'encoder.layers.6.linear1.bias', 'encoder.layers.19.norm1.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.1.linear2.weight', 'encoder.layers.3.norm2.bias', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.1.linear2.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.5.norm1.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.6.linear2.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.10.norm1.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.13.linear2.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.4.norm2.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.7.norm2.bias', 'encoder.layers.18.linear2.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.12.linear2.weight', 'encoder.layers.15.norm2.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.6.linear1.weight', 'encoder.layers.17.norm1.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.5.linear2.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.13.linear2.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.11.norm2.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.16.self_attn.out_proj.weight', 'embeddings.layer_norm.bias', 'encoder.layers.16.norm2.bias', 'encoder.layers.7.linear1.bias', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.7.norm1.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'embeddings.layer_norm.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.5.linear1.weight', 'encoder.layers.13.linear1.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.15.norm1.bias', 'encoder.layers.16.norm2.weight', 'encoder.layers.15.linear1.weight', 'encoder.layers.15.linear1.bias', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.17.linear2.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.3.norm2.weight', 'encoder.layers.13.norm2.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.7.linear1.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.2.linear1.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.7.norm1.bias', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.19.linear2.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.1.norm1.bias', 'encoder.layers.19.self_attn.out_proj.weight', 'embeddings.word_embeddings.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.12.linear1.bias', 'encoder.layers.19.linear2.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.16.norm1.bias', 'encoder.layers.6.norm2.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.12.norm2.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.1.linear1.weight', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.12.norm2.weight', 'encoder.layers.17.linear1.bias', 'encoder.layers.1.linear1.bias', 'encoder.layers.3.linear2.weight', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.18.norm1.weight', 'encoder.layers.6.norm1.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.11.norm2.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.16.norm1.weight', 'encoder.layers.17.norm2.bias', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.17.norm2.weight', 'encoder.layers.19.linear1.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.3.norm1.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.18.linear1.weight', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.6.norm1.bias', 'encoder.layers.18.norm2.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.14.norm2.weight', 'encoder.layers.14.linear1.weight', 'encoder.layers.17.linear2.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.15.norm1.weight', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'embeddings.task_type_embeddings.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.9.norm1.bias', 'encoder.layers.2.norm1.bias', 'encoder.layers.9.linear2.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.10.linear2.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.4.norm1.bias', 'encoder.layers.13.norm1.weight', 'encoder.layers.8.linear1.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.19.norm1.weight', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.7.linear2.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.13.norm2.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.13.linear1.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.10.linear2.bias', 'encoder.layers.18.linear2.weight', 'encoder.layers.9.norm1.weight', 'encoder.layers.2.linear2.weight', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.11.linear2.bias', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.5.linear2.weight', 'encoder.layers.13.norm1.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.2.linear2.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.10.linear1.bias', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.10.linear1.weight', 'encoder.layers.15.linear2.weight', 'encoder.layers.2.linear1.bias', 'encoder.layers.6.linear2.weight', 'encoder.layers.4.norm1.weight', 'encoder.layers.8.linear1.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.16.linear1.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.10.norm1.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.3.linear2.bias', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.16.linear2.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 108, in <module>
    text_list = gen_text_file_from_json(args.input_json_file)
  File "/home/aistudio/work/model/data.py", line 155, in gen_text_file_from_json
    with open(input_json_file, "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: ''
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-20 09:08:06,931] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 09:08:06,957] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 09:08:06,957] [    INFO][0m - Loading configuration file model_checkpoints/model_2000/config.json[0m
[32m[2024-06-20 09:08:06,958] [    INFO][0m - Loading weights file model_checkpoints/model_2000/model_state.pdparams[0m
[32m[2024-06-20 09:08:10,643] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0620 09:08:10.647897 20477 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0620 09:08:10.649473 20477 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-20 09:08:17,589] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_2000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-20 09:08:17,589] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_2000 and are newly initialized: ['encoder.layers.2.linear2.weight', 'encoder.layers.12.norm1.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.11.norm1.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.17.linear2.weight', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.7.norm2.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.15.norm1.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.17.linear1.weight', 'encoder.layers.0.norm1.weight', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'embeddings.word_embeddings.weight', 'encoder.layers.4.norm1.weight', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.16.norm2.weight', 'encoder.layers.17.norm2.bias', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.11.norm2.bias', 'encoder.layers.4.linear2.bias', 'encoder.layers.5.linear1.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.18.self_attn.k_proj.weight', 'embeddings.layer_norm.weight', 'encoder.layers.5.linear2.bias', 'encoder.layers.7.norm1.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.18.norm2.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.18.linear2.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.14.linear2.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.3.linear2.weight', 'pooler.dense.bias', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.9.norm1.weight', 'encoder.layers.6.linear2.bias', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.13.norm2.bias', 'encoder.layers.9.norm2.bias', 'encoder.layers.18.linear1.weight', 'encoder.layers.5.norm1.weight', 'encoder.layers.11.linear2.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.14.linear2.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.15.norm2.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.8.norm2.bias', 'encoder.layers.14.norm1.weight', 'encoder.layers.17.norm1.weight', 'encoder.layers.2.norm2.weight', 'encoder.layers.10.norm1.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'pooler.dense.weight', 'embeddings.task_type_embeddings.weight', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.7.linear2.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.13.norm1.weight', 'encoder.layers.13.linear2.weight', 'encoder.layers.7.linear1.bias', 'encoder.layers.1.linear1.weight', 'encoder.layers.9.norm2.weight', 'encoder.layers.19.linear1.bias', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.8.norm1.weight', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.18.norm2.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.19.linear2.weight', 'encoder.layers.19.norm1.bias', 'encoder.layers.10.linear2.weight', 'encoder.layers.1.linear2.weight', 'encoder.layers.4.linear1.weight', 'encoder.layers.4.norm2.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.14.linear1.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.2.linear1.bias', 'encoder.layers.9.linear1.bias', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.3.norm1.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.15.linear2.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.1.linear1.bias', 'encoder.layers.2.norm1.bias', 'encoder.layers.7.norm2.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.1.linear2.bias', 'encoder.layers.17.linear2.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.16.norm1.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.16.norm1.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.0.linear1.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.5.linear1.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.4.norm2.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.12.linear2.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.13.norm1.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.12.norm2.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.19.self_attn.k_proj.weight', 'embeddings.layer_norm.bias', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.8.linear1.weight', 'encoder.layers.14.norm1.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.13.norm2.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.11.norm1.weight', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.19.linear1.weight', 'encoder.layers.2.linear2.bias', 'encoder.layers.1.norm1.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.17.norm1.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.14.linear1.weight', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.2.linear1.weight', 'encoder.layers.6.linear1.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.6.norm2.bias', 'encoder.layers.16.linear1.bias', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.0.linear1.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.16.linear2.bias', 'encoder.layers.15.linear1.bias', 'encoder.layers.17.linear1.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.19.norm2.weight', 'encoder.layers.15.norm1.weight', 'encoder.layers.9.linear2.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.11.linear1.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.10.linear2.bias', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.7.linear2.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.3.norm2.bias', 'encoder.layers.6.linear2.weight', 'encoder.layers.8.linear2.weight', 'encoder.layers.10.norm2.weight', 'encoder.layers.9.linear1.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.18.linear2.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.11.linear2.bias', 'encoder.layers.4.norm1.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.5.norm2.bias', 'encoder.layers.16.linear1.weight', 'encoder.layers.19.norm1.weight', 'encoder.layers.17.norm2.weight', 'encoder.layers.18.linear1.bias', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.16.linear2.weight', 'encoder.layers.8.linear1.bias', 'encoder.layers.13.linear1.bias', 'encoder.layers.0.norm1.bias', 'encoder.layers.13.linear2.bias', 'encoder.layers.11.linear1.weight', 'encoder.layers.5.linear2.weight', 'encoder.layers.0.linear2.bias', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.14.norm2.weight', 'encoder.layers.8.linear2.bias', 'encoder.layers.19.norm2.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.6.norm1.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.norm2.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.7.norm1.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.6.linear1.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.13.linear1.weight', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.10.linear1.weight', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.3.norm1.bias', 'encoder.layers.12.linear2.weight', 'embeddings.position_embeddings.weight', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.3.linear2.bias', 'encoder.layers.4.linear1.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.12.norm1.bias', 'encoder.layers.14.norm2.bias', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.7.linear1.weight', 'encoder.layers.18.norm1.bias', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.10.linear1.bias', 'encoder.layers.9.linear2.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.16.norm2.bias', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.15.linear2.bias', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.0.linear2.weight', 'encoder.layers.11.norm2.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.12.linear1.bias', 'encoder.layers.19.linear2.bias', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.6.norm1.weight', 'encoder.layers.0.norm2.bias', 'encoder.layers.4.linear2.weight', 'encoder.layers.10.norm1.weight', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.12.norm2.bias', 'encoder.layers.10.self_attn.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
Traceback (most recent call last):
  File "/home/aistudio/work/model/infer_and_search.py", line 108, in <module>
    text_list = gen_text_file_from_json(args.input_json_file)
  File "/home/aistudio/work/model/data.py", line 155, in gen_text_file_from_json
    with open(input_json_file, "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: '../data/dev.json'
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-20 09:08:58,746] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 09:08:58,774] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 09:08:58,774] [    INFO][0m - Loading configuration file model_checkpoints/model_2000/config.json[0m
[32m[2024-06-20 09:08:58,775] [    INFO][0m - Loading weights file model_checkpoints/model_2000/model_state.pdparams[0m
[32m[2024-06-20 09:09:02,067] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0620 09:09:02.070962 22054 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0620 09:09:02.072278 22054 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-20 09:09:08,242] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_2000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-20 09:09:08,242] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_2000 and are newly initialized: ['encoder.layers.8.norm2.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.0.norm2.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.18.norm2.weight', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.11.linear1.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.2.linear2.bias', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.16.linear1.weight', 'encoder.layers.10.linear1.weight', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.8.linear1.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.11.norm1.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.14.norm2.bias', 'encoder.layers.1.linear2.bias', 'encoder.layers.17.norm2.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layers.8.linear2.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.14.linear1.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.9.linear2.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.5.linear2.weight', 'encoder.layers.3.linear2.weight', 'encoder.layers.6.norm1.weight', 'encoder.layers.19.norm2.bias', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.5.norm1.weight', 'encoder.layers.8.norm1.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.7.norm2.weight', 'embeddings.task_type_embeddings.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.16.linear1.bias', 'encoder.layers.2.linear2.weight', 'encoder.layers.13.linear2.weight', 'encoder.layers.10.norm1.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.0.linear2.bias', 'encoder.layers.17.linear2.weight', 'encoder.layers.13.norm1.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.13.norm2.weight', 'encoder.layers.1.linear1.weight', 'encoder.layers.19.linear2.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.13.norm1.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.15.norm1.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.3.linear1.bias', 'encoder.layers.3.linear2.bias', 'embeddings.word_embeddings.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.11.norm2.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.18.norm1.bias', 'encoder.layers.2.norm1.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.12.norm2.bias', 'encoder.layers.6.norm1.bias', 'encoder.layers.9.norm2.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.14.linear1.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.0.norm1.bias', 'encoder.layers.12.linear2.weight', 'encoder.layers.16.norm2.weight', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.2.norm1.weight', 'embeddings.layer_norm.weight', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.6.linear1.bias', 'encoder.layers.16.linear2.weight', 'encoder.layers.19.norm2.weight', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.15.linear2.bias', 'encoder.layers.5.linear2.bias', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.0.linear1.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.6.linear2.bias', 'embeddings.position_embeddings.weight', 'encoder.layers.7.linear1.bias', 'encoder.layers.11.norm2.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.17.linear2.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.12.linear1.bias', 'encoder.layers.7.norm1.bias', 'encoder.layers.2.linear1.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.13.linear1.weight', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.17.norm1.bias', 'encoder.layers.5.norm1.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.12.norm1.weight', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.8.linear2.weight', 'encoder.layers.3.linear1.weight', 'encoder.layers.15.norm1.bias', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.5.norm2.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.7.linear2.weight', 'encoder.layers.12.linear1.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.4.norm1.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.8.linear1.bias', 'encoder.layers.9.linear1.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.13.norm2.bias', 'encoder.layers.14.norm1.weight', 'encoder.layers.1.norm1.bias', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.7.linear2.bias', 'encoder.layers.5.norm2.bias', 'encoder.layers.12.linear2.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.14.linear2.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.4.linear2.weight', 'encoder.layers.7.norm2.bias', 'encoder.layers.9.norm1.weight', 'encoder.layers.16.norm1.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.2.linear1.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.10.linear2.bias', 'encoder.layers.6.linear2.weight', 'encoder.layers.7.linear1.weight', 'encoder.layers.7.norm1.weight', 'encoder.layers.15.linear1.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.19.self_attn.out_proj.weight', 'pooler.dense.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.13.linear2.bias', 'encoder.layers.3.norm1.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.15.linear2.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.5.linear1.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.19.linear2.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.norm1.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.17.norm1.weight', 'encoder.layers.12.norm2.weight', 'encoder.layers.6.linear1.weight', 'encoder.layers.19.norm1.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.10.linear1.bias', 'encoder.layers.11.linear2.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.16.linear2.bias', 'encoder.layers.15.linear1.bias', 'encoder.layers.9.norm2.bias', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.10.linear2.weight', 'embeddings.layer_norm.bias', 'encoder.layers.19.norm1.weight', 'encoder.layers.3.norm2.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.14.norm2.weight', 'encoder.layers.18.linear1.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.9.linear2.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.13.linear1.bias', 'encoder.layers.6.norm2.bias', 'encoder.layers.19.linear1.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.18.linear2.weight', 'encoder.layers.12.norm1.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.11.linear2.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.4.norm2.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.11.linear1.bias', 'encoder.layers.15.norm2.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.norm2.weight', 'encoder.layers.5.linear1.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.9.linear1.weight', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.16.norm1.bias', 'encoder.layers.14.linear2.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.16.norm2.bias', 'encoder.layers.17.linear1.bias', 'encoder.layers.1.linear2.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.19.linear1.weight', 'encoder.layers.17.linear1.weight', 'encoder.layers.18.linear2.bias', 'encoder.layers.19.self_attn.k_proj.bias', 'pooler.dense.weight', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.11.norm1.weight', 'encoder.layers.4.linear1.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.10.norm1.weight', 'encoder.layers.1.norm2.weight', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.18.norm2.bias', 'encoder.layers.14.norm1.bias', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.10.norm2.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.4.linear2.bias', 'encoder.layers.8.norm1.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.17.norm2.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.18.linear1.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.1.linear1.bias', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.12.self_attn.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-06-20 09:20:49,525] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 09:20:49,565] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'model_checkpoints/model_2000'.[0m
[32m[2024-06-20 09:20:49,565] [    INFO][0m - Loading configuration file model_checkpoints/model_2000/config.json[0m
[32m[2024-06-20 09:20:49,566] [    INFO][0m - Loading weights file model_checkpoints/model_2000/model_state.pdparams[0m
[32m[2024-06-20 09:20:52,795] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0620 09:20:52.799345 42537 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0620 09:20:52.800738 42537 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-06-20 09:20:59,163] [ WARNING][0m - Some weights of the model checkpoint at model_checkpoints/model_2000 were not used when initializing ErnieModel: ['emb_reduce_linear.bias', 'emb_reduce_linear.weight', 'ptm.embeddings.layer_norm.bias', 'ptm.embeddings.layer_norm.weight', 'ptm.embeddings.position_embeddings.weight', 'ptm.embeddings.task_type_embeddings.weight', 'ptm.embeddings.token_type_embeddings.weight', 'ptm.embeddings.word_embeddings.weight', 'ptm.encoder.layers.0.linear1.bias', 'ptm.encoder.layers.0.linear1.weight', 'ptm.encoder.layers.0.linear2.bias', 'ptm.encoder.layers.0.linear2.weight', 'ptm.encoder.layers.0.norm1.bias', 'ptm.encoder.layers.0.norm1.weight', 'ptm.encoder.layers.0.norm2.bias', 'ptm.encoder.layers.0.norm2.weight', 'ptm.encoder.layers.0.self_attn.k_proj.bias', 'ptm.encoder.layers.0.self_attn.k_proj.weight', 'ptm.encoder.layers.0.self_attn.out_proj.bias', 'ptm.encoder.layers.0.self_attn.out_proj.weight', 'ptm.encoder.layers.0.self_attn.q_proj.bias', 'ptm.encoder.layers.0.self_attn.q_proj.weight', 'ptm.encoder.layers.0.self_attn.v_proj.bias', 'ptm.encoder.layers.0.self_attn.v_proj.weight', 'ptm.encoder.layers.1.linear1.bias', 'ptm.encoder.layers.1.linear1.weight', 'ptm.encoder.layers.1.linear2.bias', 'ptm.encoder.layers.1.linear2.weight', 'ptm.encoder.layers.1.norm1.bias', 'ptm.encoder.layers.1.norm1.weight', 'ptm.encoder.layers.1.norm2.bias', 'ptm.encoder.layers.1.norm2.weight', 'ptm.encoder.layers.1.self_attn.k_proj.bias', 'ptm.encoder.layers.1.self_attn.k_proj.weight', 'ptm.encoder.layers.1.self_attn.out_proj.bias', 'ptm.encoder.layers.1.self_attn.out_proj.weight', 'ptm.encoder.layers.1.self_attn.q_proj.bias', 'ptm.encoder.layers.1.self_attn.q_proj.weight', 'ptm.encoder.layers.1.self_attn.v_proj.bias', 'ptm.encoder.layers.1.self_attn.v_proj.weight', 'ptm.encoder.layers.10.linear1.bias', 'ptm.encoder.layers.10.linear1.weight', 'ptm.encoder.layers.10.linear2.bias', 'ptm.encoder.layers.10.linear2.weight', 'ptm.encoder.layers.10.norm1.bias', 'ptm.encoder.layers.10.norm1.weight', 'ptm.encoder.layers.10.norm2.bias', 'ptm.encoder.layers.10.norm2.weight', 'ptm.encoder.layers.10.self_attn.k_proj.bias', 'ptm.encoder.layers.10.self_attn.k_proj.weight', 'ptm.encoder.layers.10.self_attn.out_proj.bias', 'ptm.encoder.layers.10.self_attn.out_proj.weight', 'ptm.encoder.layers.10.self_attn.q_proj.bias', 'ptm.encoder.layers.10.self_attn.q_proj.weight', 'ptm.encoder.layers.10.self_attn.v_proj.bias', 'ptm.encoder.layers.10.self_attn.v_proj.weight', 'ptm.encoder.layers.11.linear1.bias', 'ptm.encoder.layers.11.linear1.weight', 'ptm.encoder.layers.11.linear2.bias', 'ptm.encoder.layers.11.linear2.weight', 'ptm.encoder.layers.11.norm1.bias', 'ptm.encoder.layers.11.norm1.weight', 'ptm.encoder.layers.11.norm2.bias', 'ptm.encoder.layers.11.norm2.weight', 'ptm.encoder.layers.11.self_attn.k_proj.bias', 'ptm.encoder.layers.11.self_attn.k_proj.weight', 'ptm.encoder.layers.11.self_attn.out_proj.bias', 'ptm.encoder.layers.11.self_attn.out_proj.weight', 'ptm.encoder.layers.11.self_attn.q_proj.bias', 'ptm.encoder.layers.11.self_attn.q_proj.weight', 'ptm.encoder.layers.11.self_attn.v_proj.bias', 'ptm.encoder.layers.11.self_attn.v_proj.weight', 'ptm.encoder.layers.12.linear1.bias', 'ptm.encoder.layers.12.linear1.weight', 'ptm.encoder.layers.12.linear2.bias', 'ptm.encoder.layers.12.linear2.weight', 'ptm.encoder.layers.12.norm1.bias', 'ptm.encoder.layers.12.norm1.weight', 'ptm.encoder.layers.12.norm2.bias', 'ptm.encoder.layers.12.norm2.weight', 'ptm.encoder.layers.12.self_attn.k_proj.bias', 'ptm.encoder.layers.12.self_attn.k_proj.weight', 'ptm.encoder.layers.12.self_attn.out_proj.bias', 'ptm.encoder.layers.12.self_attn.out_proj.weight', 'ptm.encoder.layers.12.self_attn.q_proj.bias', 'ptm.encoder.layers.12.self_attn.q_proj.weight', 'ptm.encoder.layers.12.self_attn.v_proj.bias', 'ptm.encoder.layers.12.self_attn.v_proj.weight', 'ptm.encoder.layers.13.linear1.bias', 'ptm.encoder.layers.13.linear1.weight', 'ptm.encoder.layers.13.linear2.bias', 'ptm.encoder.layers.13.linear2.weight', 'ptm.encoder.layers.13.norm1.bias', 'ptm.encoder.layers.13.norm1.weight', 'ptm.encoder.layers.13.norm2.bias', 'ptm.encoder.layers.13.norm2.weight', 'ptm.encoder.layers.13.self_attn.k_proj.bias', 'ptm.encoder.layers.13.self_attn.k_proj.weight', 'ptm.encoder.layers.13.self_attn.out_proj.bias', 'ptm.encoder.layers.13.self_attn.out_proj.weight', 'ptm.encoder.layers.13.self_attn.q_proj.bias', 'ptm.encoder.layers.13.self_attn.q_proj.weight', 'ptm.encoder.layers.13.self_attn.v_proj.bias', 'ptm.encoder.layers.13.self_attn.v_proj.weight', 'ptm.encoder.layers.14.linear1.bias', 'ptm.encoder.layers.14.linear1.weight', 'ptm.encoder.layers.14.linear2.bias', 'ptm.encoder.layers.14.linear2.weight', 'ptm.encoder.layers.14.norm1.bias', 'ptm.encoder.layers.14.norm1.weight', 'ptm.encoder.layers.14.norm2.bias', 'ptm.encoder.layers.14.norm2.weight', 'ptm.encoder.layers.14.self_attn.k_proj.bias', 'ptm.encoder.layers.14.self_attn.k_proj.weight', 'ptm.encoder.layers.14.self_attn.out_proj.bias', 'ptm.encoder.layers.14.self_attn.out_proj.weight', 'ptm.encoder.layers.14.self_attn.q_proj.bias', 'ptm.encoder.layers.14.self_attn.q_proj.weight', 'ptm.encoder.layers.14.self_attn.v_proj.bias', 'ptm.encoder.layers.14.self_attn.v_proj.weight', 'ptm.encoder.layers.15.linear1.bias', 'ptm.encoder.layers.15.linear1.weight', 'ptm.encoder.layers.15.linear2.bias', 'ptm.encoder.layers.15.linear2.weight', 'ptm.encoder.layers.15.norm1.bias', 'ptm.encoder.layers.15.norm1.weight', 'ptm.encoder.layers.15.norm2.bias', 'ptm.encoder.layers.15.norm2.weight', 'ptm.encoder.layers.15.self_attn.k_proj.bias', 'ptm.encoder.layers.15.self_attn.k_proj.weight', 'ptm.encoder.layers.15.self_attn.out_proj.bias', 'ptm.encoder.layers.15.self_attn.out_proj.weight', 'ptm.encoder.layers.15.self_attn.q_proj.bias', 'ptm.encoder.layers.15.self_attn.q_proj.weight', 'ptm.encoder.layers.15.self_attn.v_proj.bias', 'ptm.encoder.layers.15.self_attn.v_proj.weight', 'ptm.encoder.layers.16.linear1.bias', 'ptm.encoder.layers.16.linear1.weight', 'ptm.encoder.layers.16.linear2.bias', 'ptm.encoder.layers.16.linear2.weight', 'ptm.encoder.layers.16.norm1.bias', 'ptm.encoder.layers.16.norm1.weight', 'ptm.encoder.layers.16.norm2.bias', 'ptm.encoder.layers.16.norm2.weight', 'ptm.encoder.layers.16.self_attn.k_proj.bias', 'ptm.encoder.layers.16.self_attn.k_proj.weight', 'ptm.encoder.layers.16.self_attn.out_proj.bias', 'ptm.encoder.layers.16.self_attn.out_proj.weight', 'ptm.encoder.layers.16.self_attn.q_proj.bias', 'ptm.encoder.layers.16.self_attn.q_proj.weight', 'ptm.encoder.layers.16.self_attn.v_proj.bias', 'ptm.encoder.layers.16.self_attn.v_proj.weight', 'ptm.encoder.layers.17.linear1.bias', 'ptm.encoder.layers.17.linear1.weight', 'ptm.encoder.layers.17.linear2.bias', 'ptm.encoder.layers.17.linear2.weight', 'ptm.encoder.layers.17.norm1.bias', 'ptm.encoder.layers.17.norm1.weight', 'ptm.encoder.layers.17.norm2.bias', 'ptm.encoder.layers.17.norm2.weight', 'ptm.encoder.layers.17.self_attn.k_proj.bias', 'ptm.encoder.layers.17.self_attn.k_proj.weight', 'ptm.encoder.layers.17.self_attn.out_proj.bias', 'ptm.encoder.layers.17.self_attn.out_proj.weight', 'ptm.encoder.layers.17.self_attn.q_proj.bias', 'ptm.encoder.layers.17.self_attn.q_proj.weight', 'ptm.encoder.layers.17.self_attn.v_proj.bias', 'ptm.encoder.layers.17.self_attn.v_proj.weight', 'ptm.encoder.layers.18.linear1.bias', 'ptm.encoder.layers.18.linear1.weight', 'ptm.encoder.layers.18.linear2.bias', 'ptm.encoder.layers.18.linear2.weight', 'ptm.encoder.layers.18.norm1.bias', 'ptm.encoder.layers.18.norm1.weight', 'ptm.encoder.layers.18.norm2.bias', 'ptm.encoder.layers.18.norm2.weight', 'ptm.encoder.layers.18.self_attn.k_proj.bias', 'ptm.encoder.layers.18.self_attn.k_proj.weight', 'ptm.encoder.layers.18.self_attn.out_proj.bias', 'ptm.encoder.layers.18.self_attn.out_proj.weight', 'ptm.encoder.layers.18.self_attn.q_proj.bias', 'ptm.encoder.layers.18.self_attn.q_proj.weight', 'ptm.encoder.layers.18.self_attn.v_proj.bias', 'ptm.encoder.layers.18.self_attn.v_proj.weight', 'ptm.encoder.layers.19.linear1.bias', 'ptm.encoder.layers.19.linear1.weight', 'ptm.encoder.layers.19.linear2.bias', 'ptm.encoder.layers.19.linear2.weight', 'ptm.encoder.layers.19.norm1.bias', 'ptm.encoder.layers.19.norm1.weight', 'ptm.encoder.layers.19.norm2.bias', 'ptm.encoder.layers.19.norm2.weight', 'ptm.encoder.layers.19.self_attn.k_proj.bias', 'ptm.encoder.layers.19.self_attn.k_proj.weight', 'ptm.encoder.layers.19.self_attn.out_proj.bias', 'ptm.encoder.layers.19.self_attn.out_proj.weight', 'ptm.encoder.layers.19.self_attn.q_proj.bias', 'ptm.encoder.layers.19.self_attn.q_proj.weight', 'ptm.encoder.layers.19.self_attn.v_proj.bias', 'ptm.encoder.layers.19.self_attn.v_proj.weight', 'ptm.encoder.layers.2.linear1.bias', 'ptm.encoder.layers.2.linear1.weight', 'ptm.encoder.layers.2.linear2.bias', 'ptm.encoder.layers.2.linear2.weight', 'ptm.encoder.layers.2.norm1.bias', 'ptm.encoder.layers.2.norm1.weight', 'ptm.encoder.layers.2.norm2.bias', 'ptm.encoder.layers.2.norm2.weight', 'ptm.encoder.layers.2.self_attn.k_proj.bias', 'ptm.encoder.layers.2.self_attn.k_proj.weight', 'ptm.encoder.layers.2.self_attn.out_proj.bias', 'ptm.encoder.layers.2.self_attn.out_proj.weight', 'ptm.encoder.layers.2.self_attn.q_proj.bias', 'ptm.encoder.layers.2.self_attn.q_proj.weight', 'ptm.encoder.layers.2.self_attn.v_proj.bias', 'ptm.encoder.layers.2.self_attn.v_proj.weight', 'ptm.encoder.layers.3.linear1.bias', 'ptm.encoder.layers.3.linear1.weight', 'ptm.encoder.layers.3.linear2.bias', 'ptm.encoder.layers.3.linear2.weight', 'ptm.encoder.layers.3.norm1.bias', 'ptm.encoder.layers.3.norm1.weight', 'ptm.encoder.layers.3.norm2.bias', 'ptm.encoder.layers.3.norm2.weight', 'ptm.encoder.layers.3.self_attn.k_proj.bias', 'ptm.encoder.layers.3.self_attn.k_proj.weight', 'ptm.encoder.layers.3.self_attn.out_proj.bias', 'ptm.encoder.layers.3.self_attn.out_proj.weight', 'ptm.encoder.layers.3.self_attn.q_proj.bias', 'ptm.encoder.layers.3.self_attn.q_proj.weight', 'ptm.encoder.layers.3.self_attn.v_proj.bias', 'ptm.encoder.layers.3.self_attn.v_proj.weight', 'ptm.encoder.layers.4.linear1.bias', 'ptm.encoder.layers.4.linear1.weight', 'ptm.encoder.layers.4.linear2.bias', 'ptm.encoder.layers.4.linear2.weight', 'ptm.encoder.layers.4.norm1.bias', 'ptm.encoder.layers.4.norm1.weight', 'ptm.encoder.layers.4.norm2.bias', 'ptm.encoder.layers.4.norm2.weight', 'ptm.encoder.layers.4.self_attn.k_proj.bias', 'ptm.encoder.layers.4.self_attn.k_proj.weight', 'ptm.encoder.layers.4.self_attn.out_proj.bias', 'ptm.encoder.layers.4.self_attn.out_proj.weight', 'ptm.encoder.layers.4.self_attn.q_proj.bias', 'ptm.encoder.layers.4.self_attn.q_proj.weight', 'ptm.encoder.layers.4.self_attn.v_proj.bias', 'ptm.encoder.layers.4.self_attn.v_proj.weight', 'ptm.encoder.layers.5.linear1.bias', 'ptm.encoder.layers.5.linear1.weight', 'ptm.encoder.layers.5.linear2.bias', 'ptm.encoder.layers.5.linear2.weight', 'ptm.encoder.layers.5.norm1.bias', 'ptm.encoder.layers.5.norm1.weight', 'ptm.encoder.layers.5.norm2.bias', 'ptm.encoder.layers.5.norm2.weight', 'ptm.encoder.layers.5.self_attn.k_proj.bias', 'ptm.encoder.layers.5.self_attn.k_proj.weight', 'ptm.encoder.layers.5.self_attn.out_proj.bias', 'ptm.encoder.layers.5.self_attn.out_proj.weight', 'ptm.encoder.layers.5.self_attn.q_proj.bias', 'ptm.encoder.layers.5.self_attn.q_proj.weight', 'ptm.encoder.layers.5.self_attn.v_proj.bias', 'ptm.encoder.layers.5.self_attn.v_proj.weight', 'ptm.encoder.layers.6.linear1.bias', 'ptm.encoder.layers.6.linear1.weight', 'ptm.encoder.layers.6.linear2.bias', 'ptm.encoder.layers.6.linear2.weight', 'ptm.encoder.layers.6.norm1.bias', 'ptm.encoder.layers.6.norm1.weight', 'ptm.encoder.layers.6.norm2.bias', 'ptm.encoder.layers.6.norm2.weight', 'ptm.encoder.layers.6.self_attn.k_proj.bias', 'ptm.encoder.layers.6.self_attn.k_proj.weight', 'ptm.encoder.layers.6.self_attn.out_proj.bias', 'ptm.encoder.layers.6.self_attn.out_proj.weight', 'ptm.encoder.layers.6.self_attn.q_proj.bias', 'ptm.encoder.layers.6.self_attn.q_proj.weight', 'ptm.encoder.layers.6.self_attn.v_proj.bias', 'ptm.encoder.layers.6.self_attn.v_proj.weight', 'ptm.encoder.layers.7.linear1.bias', 'ptm.encoder.layers.7.linear1.weight', 'ptm.encoder.layers.7.linear2.bias', 'ptm.encoder.layers.7.linear2.weight', 'ptm.encoder.layers.7.norm1.bias', 'ptm.encoder.layers.7.norm1.weight', 'ptm.encoder.layers.7.norm2.bias', 'ptm.encoder.layers.7.norm2.weight', 'ptm.encoder.layers.7.self_attn.k_proj.bias', 'ptm.encoder.layers.7.self_attn.k_proj.weight', 'ptm.encoder.layers.7.self_attn.out_proj.bias', 'ptm.encoder.layers.7.self_attn.out_proj.weight', 'ptm.encoder.layers.7.self_attn.q_proj.bias', 'ptm.encoder.layers.7.self_attn.q_proj.weight', 'ptm.encoder.layers.7.self_attn.v_proj.bias', 'ptm.encoder.layers.7.self_attn.v_proj.weight', 'ptm.encoder.layers.8.linear1.bias', 'ptm.encoder.layers.8.linear1.weight', 'ptm.encoder.layers.8.linear2.bias', 'ptm.encoder.layers.8.linear2.weight', 'ptm.encoder.layers.8.norm1.bias', 'ptm.encoder.layers.8.norm1.weight', 'ptm.encoder.layers.8.norm2.bias', 'ptm.encoder.layers.8.norm2.weight', 'ptm.encoder.layers.8.self_attn.k_proj.bias', 'ptm.encoder.layers.8.self_attn.k_proj.weight', 'ptm.encoder.layers.8.self_attn.out_proj.bias', 'ptm.encoder.layers.8.self_attn.out_proj.weight', 'ptm.encoder.layers.8.self_attn.q_proj.bias', 'ptm.encoder.layers.8.self_attn.q_proj.weight', 'ptm.encoder.layers.8.self_attn.v_proj.bias', 'ptm.encoder.layers.8.self_attn.v_proj.weight', 'ptm.encoder.layers.9.linear1.bias', 'ptm.encoder.layers.9.linear1.weight', 'ptm.encoder.layers.9.linear2.bias', 'ptm.encoder.layers.9.linear2.weight', 'ptm.encoder.layers.9.norm1.bias', 'ptm.encoder.layers.9.norm1.weight', 'ptm.encoder.layers.9.norm2.bias', 'ptm.encoder.layers.9.norm2.weight', 'ptm.encoder.layers.9.self_attn.k_proj.bias', 'ptm.encoder.layers.9.self_attn.k_proj.weight', 'ptm.encoder.layers.9.self_attn.out_proj.bias', 'ptm.encoder.layers.9.self_attn.out_proj.weight', 'ptm.encoder.layers.9.self_attn.q_proj.bias', 'ptm.encoder.layers.9.self_attn.q_proj.weight', 'ptm.encoder.layers.9.self_attn.v_proj.bias', 'ptm.encoder.layers.9.self_attn.v_proj.weight', 'ptm.pooler.dense.bias', 'ptm.pooler.dense.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2024-06-20 09:20:59,163] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at model_checkpoints/model_2000 and are newly initialized: ['encoder.layers.0.linear1.weight', 'encoder.layers.13.linear2.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.19.linear2.bias', 'encoder.layers.12.linear2.weight', 'encoder.layers.2.norm1.weight', 'encoder.layers.4.linear2.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.17.linear1.weight', 'encoder.layers.7.linear2.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.0.norm1.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.16.norm1.bias', 'encoder.layers.15.linear2.weight', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.11.norm2.weight', 'encoder.layers.18.norm1.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layers.18.norm2.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.7.linear1.bias', 'embeddings.layer_norm.weight', 'encoder.layers.8.norm1.weight', 'encoder.layers.10.norm2.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.17.norm1.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.14.norm2.weight', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.16.linear1.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.19.self_attn.k_proj.bias', 'encoder.layers.17.linear2.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.5.norm2.weight', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.11.norm2.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.2.linear2.bias', 'encoder.layers.18.norm2.bias', 'encoder.layers.16.norm1.weight', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.4.linear1.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.9.norm1.weight', 'encoder.layers.0.linear2.bias', 'encoder.layers.0.linear1.bias', 'encoder.layers.8.linear2.bias', 'encoder.layers.14.linear1.weight', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.7.norm1.weight', 'encoder.layers.2.norm2.weight', 'encoder.layers.5.norm2.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.1.linear1.weight', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.10.norm1.bias', 'encoder.layers.16.linear2.bias', 'encoder.layers.6.norm1.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.19.norm2.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.17.norm1.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.12.norm1.bias', 'encoder.layers.17.norm2.bias', 'encoder.layers.18.self_attn.k_proj.bias', 'encoder.layers.9.linear2.weight', 'encoder.layers.3.norm1.weight', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.19.norm1.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.2.norm1.bias', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.11.norm1.bias', 'encoder.layers.11.linear2.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.14.norm2.bias', 'encoder.layers.17.linear1.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.0.norm1.weight', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.12.linear1.bias', 'encoder.layers.1.linear1.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.13.linear2.bias', 'encoder.layers.6.linear2.weight', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.9.linear1.bias', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.9.norm2.weight', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.15.norm1.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.19.linear1.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.8.linear1.bias', 'encoder.layers.6.linear1.bias', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.14.linear1.bias', 'encoder.layers.5.linear1.weight', 'encoder.layers.12.self_attn.k_proj.bias', 'encoder.layers.13.norm2.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.10.norm1.weight', 'encoder.layers.7.norm2.weight', 'encoder.layers.15.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.9.norm2.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.13.linear1.weight', 'encoder.layers.3.norm2.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.12.linear2.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.16.self_attn.k_proj.bias', 'encoder.layers.16.linear1.weight', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.19.linear2.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.2.linear1.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.18.linear1.weight', 'encoder.layers.8.linear1.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.4.norm1.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.3.linear2.bias', 'encoder.layers.3.linear2.weight', 'encoder.layers.15.linear2.bias', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.4.linear1.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.19.linear1.weight', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.12.norm2.bias', 'encoder.layers.10.linear1.weight', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.8.linear2.weight', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.19.norm1.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.6.linear1.weight', 'encoder.layers.13.norm1.bias', 'encoder.layers.5.linear2.weight', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.15.norm2.weight', 'encoder.layers.0.norm2.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.3.norm2.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.13.norm1.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.16.norm2.bias', 'encoder.layers.5.norm1.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.0.norm2.bias', 'encoder.layers.12.norm2.weight', 'encoder.layers.9.linear1.weight', 'encoder.layers.2.linear2.weight', 'encoder.layers.10.linear2.bias', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.14.linear2.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.12.norm1.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.7.linear2.weight', 'encoder.layers.15.linear1.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.13.norm2.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.1.linear2.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.2.linear1.bias', 'encoder.layers.16.norm2.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.15.norm1.weight', 'encoder.layers.7.norm2.bias', 'encoder.layers.15.linear1.weight', 'encoder.layers.11.linear1.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.13.self_attn.k_proj.bias', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.14.linear2.bias', 'encoder.layers.18.linear2.bias', 'encoder.layers.12.linear1.weight', 'encoder.layers.11.norm1.weight', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.19.norm2.bias', 'encoder.layers.7.linear1.weight', 'encoder.layers.12.self_attn.out_proj.weight', 'embeddings.task_type_embeddings.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.17.norm2.weight', 'encoder.layers.4.linear2.weight', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.4.norm2.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.6.linear2.bias', 'encoder.layers.18.linear2.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.11.linear2.weight', 'encoder.layers.18.self_attn.k_proj.weight', 'pooler.dense.bias', 'encoder.layers.6.norm1.bias', 'encoder.layers.9.norm1.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.18.linear1.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.17.linear2.bias', 'embeddings.word_embeddings.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.14.norm1.weight', 'encoder.layers.11.linear1.bias', 'encoder.layers.5.norm1.weight', 'embeddings.position_embeddings.weight', 'encoder.layers.6.norm2.weight', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.k_proj.bias', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.9.linear2.bias', 'embeddings.layer_norm.bias', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.10.linear2.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.3.norm1.bias', 'encoder.layers.1.norm1.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.6.norm2.bias', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.1.linear2.weight', 'encoder.layers.5.linear2.bias', 'encoder.layers.7.norm1.bias', 'encoder.layers.10.linear1.bias', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.14.norm1.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.3.linear1.weight', 'pooler.dense.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.16.linear2.weight', 'encoder.layers.13.linear1.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
